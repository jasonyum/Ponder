# Draw u from Gaussian(0,1), i = 1,...,10 (unknown to agent) 
# arms = number of possible of actions (known to agent)
arms = 10
u = np.random.randn(arms,1) 

# Rewards of action i at time t => x_i(t) ~ N(u_i,1)
# x_i(t) is itself randomly drawn, so it's not deterministic
rounds = 5 
x = np.zeros((rounds,1))

# Initialize Q => agent's record of rewards
# Initialize estQ => agent's estimate of rewards
# We need some assumption of the initial states (some expectation), can be "optimistic" or "pessimistic"
Q = np.zeros((arms, rounds))
Q[:,0] = np.random.randn(arms) 
estQ = np.zeros((arms,1))

for time in range(rounds): 
  
    # We first cycle through each arm and estimate our belief of that arm. 
    # The belief is defined as Q_t(a) = sum of rewards prior to t / number of times action was taken.
    for i in range(arms): 
        estQ[i] = sum(Q[i,:])/np.count_nonzero(Q[i,:])
    
    chosenLever = np.argmax(estQ) # automatically picks the row w/ the highest value
    Q[chosenLever, time] = np.random.normal(u[chosenLever],1) 
        
print(Q)
print(estQ)
print(np.argmax(estQ))


